{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the presplit data\n",
    "X_train = pd.read_csv('data/X_train.csv', index_col='id')\n",
    "X_test = pd.read_csv('data/X_test.csv', index_col='id')\n",
    "y_train = pd.read_csv('data/y_train.csv', index_col='id')\n",
    "y_test = pd.read_csv('data/y_test.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 40095 entries, 45522 to 9914\n",
      "Data columns (total 31 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   amount_tsh             40095 non-null  float64\n",
      " 1   funder                 40095 non-null  object \n",
      " 2   gps_height             40095 non-null  int64  \n",
      " 3   installer              40095 non-null  object \n",
      " 4   longitude              40095 non-null  float64\n",
      " 5   latitude               40095 non-null  float64\n",
      " 6   num_private            40095 non-null  int64  \n",
      " 7   basin                  40095 non-null  object \n",
      " 8   region                 40095 non-null  object \n",
      " 9   region_code            40095 non-null  int64  \n",
      " 10  district_code          40095 non-null  int64  \n",
      " 11  population             40095 non-null  int64  \n",
      " 12  public_meeting         40095 non-null  object \n",
      " 13  scheme_management      40095 non-null  object \n",
      " 14  permit                 40095 non-null  object \n",
      " 15  construction_year      40095 non-null  int64  \n",
      " 16  extraction_type        40095 non-null  object \n",
      " 17  extraction_type_group  40095 non-null  object \n",
      " 18  extraction_type_class  40095 non-null  object \n",
      " 19  management             40095 non-null  object \n",
      " 20  management_group       40095 non-null  object \n",
      " 21  payment                40095 non-null  object \n",
      " 22  payment_type           40095 non-null  object \n",
      " 23  water_quality          40095 non-null  object \n",
      " 24  quality_group          40095 non-null  object \n",
      " 25  quantity               40095 non-null  object \n",
      " 26  source                 40095 non-null  object \n",
      " 27  source_type            40095 non-null  object \n",
      " 28  source_class           40095 non-null  object \n",
      " 29  waterpoint_type        40095 non-null  object \n",
      " 30  waterpoint_type_group  40095 non-null  object \n",
      "dtypes: float64(3), int64(6), object(22)\n",
      "memory usage: 9.8+ MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = pd.get_dummies(X_train, drop_first=True)\n",
    "\n",
    "# X_test = pd.get_dummies(X_test, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_train = y_train.reshape(40095, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)\n",
    "y_test = y_test.reshape(13365, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train_nums = X_train.select_dtypes(exclude=object)\n",
    "X_test_nums = X_test.select_dtypes(exclude=object)\n",
    "\n",
    "X_train_nums_scaled = ss.fit_transform(X_train_nums)\n",
    "X_test_nums_scaled = ss.transform(X_test_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train_nums_scaled, y_train)\n",
    "\n",
    "preds = rfc.predict(X_test_nums_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Mode's accuracy on the training data is 0.9858\n",
      "The Model's accuracy on the test data is 0.7149\n"
     ]
    }
   ],
   "source": [
    "train_scores = rfc.score(X_train_nums_scaled, y_train)\n",
    "test_scores = rfc.score(X_test_nums_scaled, y_test)\n",
    "print('The Mode\\'s accuracy on the training data is', round(train_scores, 4))\n",
    "print('The Model\\'s accuracy on the test data is', round(test_scores, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: 1.0048775599566135\n"
     ]
    }
   ],
   "source": [
    "log_loss_rfc = cross_val_score(rfc, X_train_nums_scaled, y_train, scoring='neg_log_loss', n_jobs = -1)\n",
    "log_loss_rfc = -log_loss_rfc.mean()\n",
    "print('Log Loss:', log_loss_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This inital model that only contains numerical data is severly overfitting the data. The next model will need to tune some hyperparameters to reduce the gap between the training and testing data. Our log loss has been decreased from the dummy classifer, but there is still room for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipline_1 = Pipeline([('ss', StandardScaler()), ('rfc', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('ss', StandardScaler()), ('rfc', RandomForestClassifier())],\n",
       " 'verbose': False,\n",
       " 'ss': StandardScaler(),\n",
       " 'rfc': RandomForestClassifier(),\n",
       " 'ss__copy': True,\n",
       " 'ss__with_mean': True,\n",
       " 'ss__with_std': True,\n",
       " 'rfc__bootstrap': True,\n",
       " 'rfc__ccp_alpha': 0.0,\n",
       " 'rfc__class_weight': None,\n",
       " 'rfc__criterion': 'gini',\n",
       " 'rfc__max_depth': None,\n",
       " 'rfc__max_features': 'auto',\n",
       " 'rfc__max_leaf_nodes': None,\n",
       " 'rfc__max_samples': None,\n",
       " 'rfc__min_impurity_decrease': 0.0,\n",
       " 'rfc__min_impurity_split': None,\n",
       " 'rfc__min_samples_leaf': 1,\n",
       " 'rfc__min_samples_split': 2,\n",
       " 'rfc__min_weight_fraction_leaf': 0.0,\n",
       " 'rfc__n_estimators': 100,\n",
       " 'rfc__n_jobs': None,\n",
       " 'rfc__oob_score': False,\n",
       " 'rfc__random_state': None,\n",
       " 'rfc__verbose': 0,\n",
       " 'rfc__warm_start': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipline_1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_1 = {\n",
    "    'rfc__max_depth': list(range(5,50,5)),\n",
    "    'rfc__min_samples_leaf': list(range(5,50,5)),\n",
    "    'rfc__n_estimators': list(range(25,500,25)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=Pipeline(steps=[('ss', StandardScaler()),\n",
       "                                             ('rfc',\n",
       "                                              RandomForestClassifier())]),\n",
       "                   n_iter=15, n_jobs=-1,\n",
       "                   param_distributions={'rfc__max_depth': [5, 10, 15, 20, 25,\n",
       "                                                           30, 35, 40, 45],\n",
       "                                        'rfc__min_samples_leaf': [5, 10, 15, 20,\n",
       "                                                                  25, 30, 35,\n",
       "                                                                  40, 45],\n",
       "                                        'rfc__n_estimators': [25, 50, 75, 100,\n",
       "                                                              125, 150, 175,\n",
       "                                                              200, 225, 250,\n",
       "                                                              275, 300, 325,\n",
       "                                                              350, 375, 400,\n",
       "                                                              425, 450, 475]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_1 = RandomizedSearchCV(pipline_1, params_1, n_jobs=-1, verbose=2, n_iter=10)\n",
    "\n",
    "rs_1.fit(X_train_nums, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train score for the random search with some hyperparameter tuning is 0.8255\n",
      "\n",
      "The test score for the random search with some hyperparameter tuning is 0.7217\n"
     ]
    }
   ],
   "source": [
    "print('The train score for the random search with some hyperparameter tuning is',\n",
    "      round(rs_1.score(X_train_nums, y_train), 4))\n",
    "print('')\n",
    "print('The test score for the random search with some hyperparameter tuning is',\n",
    "      round(rs_1.score(X_test_nums, y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rfc__n_estimators': 325, 'rfc__min_samples_leaf': 5, 'rfc__max_depth': 25}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss_rfc = cross_val_score(rs_1, X_train_nums_scaled, y_train, scoring='neg_log_loss', n_jobs = -1)\n",
    "log_loss_rfc = -log_loss_rfc.mean()\n",
    "print('Log Loss:', log_loss_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is he first model that will contain the categorical colums, there will be no hyperparameter tuning done yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nums = X_train.select_dtypes(exclude = object)\n",
    "\n",
    "X_train_cat = X_train.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('ss', StandardScaler())\n",
    "])\n",
    "                \n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('ohe', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(handle_unknown='ignore')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "ohe.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = ColumnTransformer(transformers=[\n",
    "    ('numerical', numerical_pipeline, X_train_nums.columns),\n",
    "    ('categorical', categorical_pipeline, X_train_cat.columns)\n",
    "], verbose=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipe = Pipeline(steps=[\n",
    "    ('trans', trans),\n",
    "    ('rfr', RandomForestClassifier(verbose=1, n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   33.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('trans',\n",
       "                 ColumnTransformer(n_jobs=-1,\n",
       "                                   transformers=[('numerical',\n",
       "                                                  Pipeline(steps=[('ss',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  Index(['amount_tsh', 'gps_height', 'longitude', 'latitude', 'num_private',\n",
       "       'region_code', 'district_code', 'population', 'construction_year'],\n",
       "      dtype='object')),\n",
       "                                                 ('categorical',\n",
       "                                                  Pipeline(steps=[('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))...\n",
       "       'scheme_management', 'permit', 'extraction_type',\n",
       "       'extraction_type_group', 'extraction_type_class', 'management',\n",
       "       'management_group', 'payment', 'payment_type', 'water_quality',\n",
       "       'quality_group', 'quantity', 'source', 'source_type', 'source_class',\n",
       "       'waterpoint_type', 'waterpoint_type_group'],\n",
       "      dtype='object'))],\n",
       "                                   verbose=True)),\n",
       "                ('rfr', RandomForestClassifier(n_jobs=-1, verbose=1))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9972565157750343"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7954358398802843"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  1.5min remaining:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8541020610545565"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_1 = cross_val_score(model_pipe, X_train, y_train, scoring='neg_log_loss', n_jobs = -1, verbose = 1)\n",
    "-cv_1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   21.7s remaining:   32.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   21.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8861059237679315"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_1 = cross_val_score(model_pipe, X_test, y_test, scoring='neg_log_loss', n_jobs = -1, verbose = 1)\n",
    "-cv_1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first model is a significant improvement from the first Random Forst Classifer model. The train score increase by $0.0736$. However, the model is still significanly overfitting and that will need to be reduced by hyperparameter tuning. \n",
    "\n",
    "The log loss also improved from $1.005$ with the first model with only numeric colums to $0.8861$ with all colums included. This was an improvement of $0.1188$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
